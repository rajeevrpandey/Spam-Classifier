{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMpJgsrUfO22Oj60n2BYWr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeevrpandey/Spam-Classifier/blob/main/Spam_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IMZfL7NujiP7"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "def fetch_spam_data():\n",
        "  spam_root = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
        "  ham_url = spam_root + \"20030228_easy_ham.tar.bz2\"\n",
        "  spam_url = spam_root + \"20030228_spam.tar.bz2\"\n",
        "\n",
        "  # creates a path for the spam datasets using the Path object from pathlib\n",
        "  spam_path = Path() / \"datasets\" / \"spam\"  # creating multiple directories\n",
        "  spam_path.mkdir(parents=True, exist_ok=True)  # mkdir method is then used to\n",
        "  #create the directory if it doesn't exist, along with any necessary parent directories (parents=True)\n",
        "\n",
        "\n",
        "  for dir_name, tar_name, url in ((\"easy_ham\", \"ham\", ham_url),\n",
        "                                  (\"spam\", \"spam\", spam_url)):\n",
        "      if not (spam_path / dir_name).is_dir():\n",
        "          #The is_dir() method returns True if the path corresponds to an existing directory, and False otherwise.\n",
        "          path = (spam_path / tar_name).with_suffix(\".tar.bz2\") #This method replaces the existing suffix of the file, if any, with the specified suffix (in this case, \".tar.bz2\")\n",
        "          print(\"Downloading\", path)\n",
        "          urllib.request.urlretrieve(url, path)\n",
        "          '''\n",
        "          urllib is a package that collects several modules for working with URLs:\n",
        "            urllib.request for opening and reading URLs\n",
        "            urllib.error containing the exceptions raised by urllib.request\n",
        "            urllib.parse for parsing URLs\n",
        "            urllib.robotparser for parsing robots.txt files\n",
        "          '''\n",
        "          tar_bz2_file = tarfile.open(path, \"r:bz2\")\n",
        "          tar_bz2_file.extractall(path=spam_path)\n",
        "          tar_bz2_file.close()\n",
        "  return [spam_path / dir_name for dir_name in (\"easy_ham\", \"spam\")]\n",
        "  # function returns a list of paths to the extracted directories for \"easy_ham\" and \"spam\" datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ham_dir, spam_dir = fetch_spam_data()"
      ],
      "metadata": {
        "id": "1d7XgmPNq1Np"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ham_filenames = [f for f in sorted(ham_dir.iterdir()) if len(f.name) > 20]\n",
        "#  \"ham_filenames\" list contains filenames from the \"ham_dir\" directory that have a length greater than 20 characters\n",
        "spam_filenames = [f for f in sorted(spam_dir.iterdir()) if len(f.name) > 20]\n",
        "# \"spam_filenames\" list contains filenames from the \"spam_dir\" directory that have a length greater than 20 characters"
      ],
      "metadata": {
        "id": "5o8FWlMks-nQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ham_filenames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQN_13tgs_go",
        "outputId": "6404a977-ab34-4b69-a593-6315a1f48afe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2500"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(spam_filenames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gWNFOvmtB0h",
        "outputId": "a2766d9a-b4b5-42d3-8ed6-2475cbd4733b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use Python's email module to parse these emails (this handles headers, encoding, and so on):\n",
        "import email  # provides the functionality to parse, create, and manipulate email messages\n",
        "import email.policy # provides policies for parsing and generating email messages\n",
        "\n",
        "def parse_email(filepath): # filepath is path to the email file\n",
        "  with open(filepath, \"rb\") as f:\n",
        "    # 'r'-read mode, 'w'- write mode, 'a'- append mode, 'r+'- read and write mode, 'w+'- write and read mode, 'a+'- append and read mode, 'b'- binary mode, 't'- text mode\n",
        "    return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
        "    # email.parser.BytesParser class is used to create a parser instance, with the parsing policy set to email.policy.default\n",
        "# parsed email message object is returned from the function"
      ],
      "metadata": {
        "id": "B6oh5SFqtEKR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ham_emails = [parse_email(filepath) for filepath in ham_filenames]\n",
        "# list of ham email message objects\n",
        "spam_emails = [parse_email(filepath) for filepath in spam_filenames]\n",
        "# list of spam email message objects"
      ],
      "metadata": {
        "id": "fjyYFrwj1jDb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at one example of ham and one example of spam, to get a feel of what the data looks like:"
      ],
      "metadata": {
        "id": "_Sx1OJ1j13Eb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ham_emails[1].get_content().strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHT00qJ62DX6",
        "outputId": "799dcbfa-e2f6-41e4-e8ee-623b886f5e69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Martin A posted:\n",
            "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
            " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
            " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
            " \n",
            " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
            " museum, a restored amphitheatre and car park for admiring crowds are\n",
            "planned\n",
            "---------------------\n",
            "So is this mountain limestone or granite?\n",
            "If it's limestone, it'll weather pretty fast.\n",
            "\n",
            "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
            "4 DVDs Free +s&p Join Now\n",
            "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
            "---------------------------------------------------------------------~->\n",
            "\n",
            "To unsubscribe from this group, send an email to:\n",
            "forteana-unsubscribe@egroups.com\n",
            "\n",
            " \n",
            "\n",
            "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spam_emails[6].get_content().strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0ySUCNJ2ZjD",
        "outputId": "41ce8067-d751-448a-f7d3-687206645fc3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
            "growing at a tremendous rate.  We are looking for individuals who\n",
            "want to work from home.\n",
            "\n",
            "This is an opportunity to make an excellent income.  No experience\n",
            "is required.  We will train you.\n",
            "\n",
            "So if you are looking to be employed from home with a career that has\n",
            "vast opportunities, then go:\n",
            "\n",
            "http://www.basetel.com/wealthnow\n",
            "\n",
            "We are looking for energetic and self motivated people.  If that is you\n",
            "than click on the link and fill out the form, and one of our\n",
            "employement specialist will contact you.\n",
            "\n",
            "To be removed from our link simple go to:\n",
            "\n",
            "http://www.basetel.com/remove.html\n",
            "\n",
            "\n",
            "4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some emails are actually multipart, with images and attachments (which can have their own attachments). Let's look at the various types of structures we have:"
      ],
      "metadata": {
        "id": "ECX2iyRj2fp8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_email_structure(email):\n",
        "  if isinstance(email, str):\n",
        "    # isinstance(email, str) verifies whether the provided input is a string.\n",
        "    # If it is, the string is returned, assuming it represents the structure of the email.\n",
        "    return email\n",
        "  # An email object payload is the data being transmitted, or the body text of an email, not including any headers.\n",
        "  # The get_payload() method is used to extract the payload of the email.\n",
        "  payload = email.get_payload()\n",
        "  if isinstance(payload, list):\n",
        "    # If the payload is a list (indicating a multipart message), the function iterates through each sub-email in the payload, recursively calling the \"get_email_structure\" function on each sub-email.\n",
        "    # The results are then joined together with a comma and labeled as a \"multipart\" message.\n",
        "    return \", \".join([get_email_structure(sub_email) for sub_email in payload])\n",
        "  else:\n",
        "    # If the payload is not a list, the function returns the content type of the email using email.get_content_type().\n",
        "    return email.get_content_type()\n",
        "  # The function returns the identified email structure, which could be a string representation of the structure or a labeled \"multipart\" structure based on the payload analysis."
      ],
      "metadata": {
        "id": "i0FgzdOU3OZF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The \"collections\" module is a built-in Python module that provides a variety of high-performance container datatypes, in addition to the built-in datatypes.\n",
        "from collections import Counter\n",
        "# Counter is used to count the occurrences of elements within an iterable or a mapping.\n",
        "\n",
        "def structures_counter(emails):\n",
        "  structures = Counter()  #  This Counter will be used to tally the occurrences of different email structures.\n",
        "  for email in emails:\n",
        "    structure = get_email_structure(email)  # email's structure is stored\n",
        "    structures[structure] += 1\n",
        "    # The occurrence of the identified email structure is then incremented within the \"structures\" Counter using the retrieved \"structure\" as the key.\n",
        "    # If the structure has not been encountered before, it is automatically initialized to 0 before being incremented.\n",
        "  return structures\n",
        "# returns a counter containing the count of different email structures observed"
      ],
      "metadata": {
        "id": "b_mPu1KoW6YC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The most_common() method is a built-in method of the Counter class that returns a list of the n most common elements and their counts from the most common to the least.\n",
        "# If no value for n is specified, it returns all elements in the Counter.\n",
        "structures_counter(ham_emails).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7lIzTdYYlZ6",
        "outputId": "3dd75525-5c4e-42a1-ad78-f5c9f24459b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('text/plain', 2411),\n",
              " ('text/plain, application/pgp-signature', 66),\n",
              " ('text/plain, text/html', 8),\n",
              " ('text/plain, text/plain', 5),\n",
              " ('text/plain, application/octet-stream', 2),\n",
              " ('text/plain, text/enriched', 1),\n",
              " ('text/plain, application/ms-tnef, text/plain', 1),\n",
              " ('text/plain, text/plain, text/plain, application/pgp-signature', 1),\n",
              " ('text/plain, video/mng', 1),\n",
              " ('text/plain, application/x-pkcs7-signature', 1),\n",
              " ('text/plain, text/plain, text/plain, text/rfc822-headers', 1),\n",
              " ('text/plain, text/plain, text/plain, text/plain, application/x-pkcs7-signature',\n",
              "  1),\n",
              " ('text/plain, application/x-java-applet', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "structures_counter(spam_emails).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UaX52pdY7p_",
        "outputId": "bc9f0984-d0f8-47da-e2e3-8789a010aee1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('text/plain', 237),\n",
              " ('text/html', 208),\n",
              " ('text/plain, text/html', 45),\n",
              " ('text/plain, image/jpeg', 3),\n",
              " ('text/html, application/octet-stream', 2),\n",
              " ('text/plain, application/octet-stream', 1),\n",
              " ('text/html, text/plain', 1),\n",
              " ('text/html, application/octet-stream, image/jpeg', 1),\n",
              " ('text/plain, text/html, image/gif', 1),\n",
              " ('multipart/alternative', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It seems that the ham emails are more often plain text, while spam has quite a lot of HTML.\n",
        "# Moreover, quite a few ham emails are signed using PGP, while no spam is.\n",
        "# In short, it seems that the email structure is useful information to have."
      ],
      "metadata": {
        "id": "eWkc1mGMZFuH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's take a look at the email headers:"
      ],
      "metadata": {
        "id": "ZhY689sOZg3Y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# items() method returns a list of 2-tuples containing all the message’s field headers and values.\n",
        "for header, value in spam_emails[0].items():\n",
        "  print(header, \":\", value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRT_fYIxZk6j",
        "outputId": "8fecd315-9fc6-41fe-ba13-a76e7371c125"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return-Path : <12a1mailbot1@web.de>\n",
            "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
            "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
            "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
            "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
            "From : 12a1mailbot1@web.de\n",
            "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
            "To : dcek1a1@netsgo.com\n",
            "Subject : Life Insurance - Why Pay More?\n",
            "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
            "MIME-Version : 1.0\n",
            "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
            "Content-Type : text/html; charset=\"iso-8859-1\"\n",
            "Content-Transfer-Encoding : quoted-printable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a lot of useful info in message’s field headers and values.\n",
        "# like subject of the above email:\n",
        "spam_emails[0][\"Subject\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TUG4iUUzajEL",
        "outputId": "bba5b78d-478c-40dd-b652-0ea0988e1dfb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Life Insurance - Why Pay More?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test:\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.array(ham_emails + spam_emails, dtype=object)\n",
        "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))  # making a new column where 0 means ham and 1 means spam\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "KNzR3-Nda3-G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import html\n",
        "\n",
        "'''\n",
        "The following function first drops the <head> section, then converts all <a> tags to the word HYPERLINK, then it gets rid of all HTML tags, leaving only the plain text.\n",
        "For readability, it also replaces multiple newlines with single newlines, and finally it unescapes html entities (such as &gt; or &nbsp;):\n",
        "'''\n",
        "\n",
        "def html_to_plain_text(html_content):\n",
        "    # Create a Beautiful Soup object\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Drop the <head> section\n",
        "    for head in soup(['head']):\n",
        "        head.decompose()\n",
        "\n",
        "    # Convert all <a> tags to the word \"HYPERLINK\"\n",
        "    for a_tag in soup('a'):\n",
        "        a_tag.string = ' HYPERLINK '\n",
        "\n",
        "    # Extract text content and remove excess whitespaces and line breaks\n",
        "    text_content = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    # Unescape HTML entities\n",
        "    text_content = html.unescape(text_content)\n",
        "\n",
        "    return text_content\n",
        "\n"
      ],
      "metadata": {
        "id": "TZa39yS9bvtL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if it works. This is HTML spam:\n",
        "\n",
        "# iterate through the emails in the training set (X_train) that are classified as spam (y_train==1) and filters for emails that have a specific structure identified as \"text/html\"\n",
        "html_spam_emails = [email for email in X_train[y_train==1] if get_email_structure(email) == \"text/html\"] # list of HTML spam emails\n",
        "sample_html_spam = html_spam_emails[5]\n",
        "# The content of the selected HTML spam email is obtained using the get_content() method.\n",
        "# The strip() method is used to remove any leading or trailing whitespace from the content, and a slice of the first 1000 characters is extracted for display.\n",
        "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI4hL3RRmyBv",
        "outputId": "8232b193-fe2f-493a-a796-1f488ec069fb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<HR>\n",
            "<html>\n",
            "<div bgcolor=\"#FFFFCC\">\n",
            "\n",
            "  <p align=\"center\"><a\n",
            "href=\"http://www.webbasedmailing.com\"><img border=\"0\"\n",
            "src=\"http://www.webbasedmailing.com/Toners2goLogo.jpg\"\n",
            "width=\"349\" height=\"96\"></a></p>\n",
            "<p align=\"center\"><font size=\"6\" face=\"Arial MT\n",
            "Black\"><i>Tremendous Savings</i>\n",
            "on Toners,&nbsp;</font></p>\n",
            "<p align=\"center\"><font size=\"6\" face=\"Arial MT\n",
            "Black\">\n",
            "Inkjets, FAX, and Thermal Replenishables!!</font></p>\n",
            "<p><a href=\"http://www.webbasedmailing.com\">Toners 2 Go\n",
            "</a>is your secret\n",
            "weapon to lowering your cost for <a\n",
            "href=\"http://www.webbasedmailing.com\">High Quality,\n",
            "Low-Cost</a> printer\n",
            "supplies!&nbsp; We have been in the printer\n",
            "replenishables business since 1992,\n",
            "and pride ourselves on rapid response and outstanding\n",
            "customer service.&nbsp;\n",
            "What we sell are 100% compatible replacements for\n",
            "Epson, Canon, Hewlett Packard,\n",
            "Xerox, Okidata, Brother, and Lexmark; products that\n",
            "meet and often exceed\n",
            "original manufacturer's specifications.</p>\n",
            "<p><i><font size=\"4\">Check out these\n",
            "p ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the resulting plain text after passing it through html_to_plain_text():\n",
        "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO_LqwE4huB6",
        "outputId": "54074987-c301-4ebb-80a8-2eb8222604cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HYPERLINK\n",
            "Tremendous Savings\n",
            "on Toners,\n",
            "Inkjets, FAX, and Thermal Replenishables!!\n",
            "HYPERLINK\n",
            "is your secret\n",
            "weapon to lowering your cost for\n",
            "HYPERLINK\n",
            "printer\n",
            "supplies!  We have been in the printer\n",
            "replenishables business since 1992,\n",
            "and pride ourselves on rapid response and outstanding\n",
            "customer service. \n",
            "What we sell are 100% compatible replacements for\n",
            "Epson, Canon, Hewlett Packard,\n",
            "Xerox, Okidata, Brother, and Lexmark; products that\n",
            "meet and often exceed\n",
            "original manufacturer's specifications.\n",
            "Check out these\n",
            "prices!\n",
            "Epson Stylus\n",
            "Color inkjet cartridge\n",
            "(SO20108):     Epson's Price:\n",
            "$27.99\n",
            "Toners2Go price: $9.95!\n",
            "HP\n",
            "LaserJet 4 Toner Cartridge\n",
            "(92298A):           \n",
            "HP's\n",
            "Price:\n",
            "$88.99\n",
            "Toners2Go\n",
            "  price: $41.75!\n",
            "Come visit us on the web to check out our hundreds\n",
            "of similar bargains at\n",
            "HYPERLINK\n",
            "!\n",
            "request to be excluded by visiting\n",
            "HYPERLINK\n",
            "beverley ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that takes an email as input and returns its content as plain text, whatever its format is:\n",
        "\n",
        "def email_to_text(email):\n",
        "    html = None\n",
        "    for part in email.walk():\n",
        "        # The walk() method is used to iterate through all the parts of a multipart message, including subparts within the message.\n",
        "        ctype = part.get_content_type()\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\n",
        "            #  If the content type is not \"text/plain\" or \"text/html\", the loop continues to the next part.\n",
        "            continue\n",
        "        try:\n",
        "            content = part.get_content()\n",
        "        except: # in case of encoding issues\n",
        "            content = str(part.get_payload()) # converted to string type\n",
        "        if ctype == \"text/plain\":\n",
        "            return content\n",
        "        else:\n",
        "            html = content\n",
        "    if html: # content is of html type therefore pass it to html_to_plain_text()\n",
        "        return html_to_plain_text(html)\n",
        ""
      ],
      "metadata": {
        "id": "BUCw3wNUkk_9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can get plain text for any format\n",
        "print(email_to_text(sample_html_spam)[:100], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7noE63ip1An",
        "outputId": "b5742ed3-cb93-4883-d67e-41ba4f6f6358"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HYPERLINK\n",
            "Tremendous Savings\n",
            "on Toners,\n",
            "Inkjets, FAX, and Thermal Replenishables!!\n",
            "HYPERLINK\n",
            "is your ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's throw in some stemming! We will use the Natural Language Toolkit (NLTK):\n",
        "# Stemming is the process of reducing words to their root or base form, known as the word stem.\n",
        "# In the context of natural language processing, stemming algorithms are used to remove suffixes and prefixes from words in order to derive their base forms.\n",
        "# PorterStemmer is best stemmer for word normalization and simplification which are essential for efficient text processing, analysis, and retrieval tasks.\n",
        "import nltk\n",
        "\n",
        "stemmer = nltk.PorterStemmer()\n",
        "for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\",\n",
        "             \"Compulsive\"):\n",
        "    print(word, \"=>\", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xXKQMR0kxPs",
        "outputId": "544810b1-2dbc-4f60-b07a-17be572b5704"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computations => comput\n",
            "Computation => comput\n",
            "Computing => comput\n",
            "Computed => comput\n",
            "Compute => comput\n",
            "Compulsive => compuls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets use the urlextract library to replace URLs with the word \"URL\"\n",
        "%pip install -q -U urlextract"
      ],
      "metadata": {
        "id": "Nt8jGtuRk90q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# urlextract libraryis used for identifying and extracting URLs from text data\n",
        "import urlextract\n",
        "\n",
        "url_extractor = urlextract.URLExtract()\n",
        "some_text = \"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"\n",
        "print(url_extractor.find_urls(some_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th11MqC3lNor",
        "outputId": "e2082a7a-ca28-41b7-dd51-fac8bd212e32"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We are ready to put all this together into a transformer that we will use to convert emails to word counters.\n",
        "Note that we split sentences into words using Python's split() method, which uses whitespaces for word boundaries.\n",
        "This works for many written languages, but not all.\n",
        "For example, Chinese and Japanese scripts generally don't use spaces between words, and Vietnamese often uses spaces even between syllables.\n",
        "It's okay here, because the dataset is (mostly) in English.\n",
        "'''\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strip_headers=True, lower_case=True,\n",
        "                 remove_punctuation=True, replace_urls=True,\n",
        "                 replace_numbers=True, stemming=True):\n",
        "        # initialising parameters to control the preprocessing steps\n",
        "        self.strip_headers = strip_headers\n",
        "        self.lower_case = lower_case\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.replace_urls = replace_urls\n",
        "        self.replace_numbers = replace_numbers\n",
        "        self.stemming = stemming\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        #  return the transformer instance\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for email in X:\n",
        "            text = email_to_text(email) or \"\"\n",
        "            if self.lower_case:\n",
        "                # email text is converted to lowercase\n",
        "                text = text.lower()\n",
        "            if self.replace_urls and url_extractor is not None:\n",
        "                # URLs are replaced with the token \"URL\"\n",
        "                urls = list(set(url_extractor.find_urls(text)))\n",
        "                urls.sort(key=lambda url: len(url), reverse=True)\n",
        "                for url in urls:\n",
        "                    text = text.replace(url, \" URL \")\n",
        "            if self.replace_numbers:\n",
        "                # Numbers are replaced with the token \"NUMBER\"\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
        "            if self.remove_punctuation:\n",
        "                # Punctuation is removed\n",
        "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "\n",
        "            # preprocessed text is split into words, and a Counter object is used to count the occurrences of each word\n",
        "            word_counts = Counter(text.split())\n",
        "\n",
        "            if self.stemming and stemmer is not None:\n",
        "                #  word counts are transformed using stemming to reduce words to their base forms\n",
        "                stemmed_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    stemmed_word = stemmer.stem(word)\n",
        "                    stemmed_word_counts[stemmed_word] += count #  increments the count of the stemmed word in the stemmed_word_counts Counter object by the original count of the word from the email text\n",
        "                word_counts = stemmed_word_counts\n",
        "            # transformed word counts, which now represent the stemmed forms of the words, are added to the X_transformed list for each email\n",
        "            X_transformed.append(word_counts)\n",
        "        return np.array(X_transformed)"
      ],
      "metadata": {
        "id": "syF2nUELlS0s"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying this transformer on a few emails:\n",
        "X_few = X_train[:3]\n",
        "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
        "X_few_wordcounts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH2TgpG3lbYb",
        "outputId": "a6256fd6-0247-404f-d4ef-9113e8b2f1df"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
              "       Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'url': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1}),\n",
              "       Counter({'url': 4, 's': 3, 'group': 3, 'to': 3, 'in': 2, 'forteana': 2, 'martin': 2, 'an': 2, 'and': 2, 'we': 2, 'is': 2, 'yahoo': 2, 'unsubscrib': 2, 'y': 1, 'adamson': 1, 'wrote': 1, 'for': 1, 'altern': 1, 'rather': 1, 'more': 1, 'factual': 1, 'base': 1, 'rundown': 1, 'on': 1, 'hamza': 1, 'career': 1, 'includ': 1, 'hi': 1, 'belief': 1, 'that': 1, 'all': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'should': 1, 'be': 1, 'murder': 1, 'outright': 1, 'know': 1, 'how': 1, 'unbias': 1, 'memri': 1, 'don': 1, 't': 1, 'html': 1, 'rob': 1, 'sponsor': 1, 'number': 1, 'dvd': 1, 'free': 1, 'p': 1, 'join': 1, 'now': 1, 'from': 1, 'thi': 1, 'send': 1, 'email': 1, 'egroup': 1, 'com': 1, 'your': 1, 'use': 1, 'of': 1, 'subject': 1})],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we have the word counts, and we need to convert them to vectors.\n",
        "# For this, we will build another transformer whose fit() method will build the vocabulary (an ordered list of the most common words)\n",
        "# and whose transform() method will use the vocabulary to convert word counts to vectors.\n",
        "# The output is a sparse matrix."
      ],
      "metadata": {
        "id": "9BN3lP9e_n8-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `enumerate()` function takes a collection (e.g. a tuple) and returns it as an enumerate object.\n",
        "The `enumerate()` function adds a counter as the key of the enumerate object.\n",
        "\n",
        "\n",
        "```\n",
        "x = ('apple', 'banana', 'cherry')\n",
        "y = enumerate(x)\n",
        "\n",
        "print(list(y))\n",
        "```\n",
        "output:\n",
        "\n",
        "```\n",
        "[(0, 'apple'), (1, 'banana'), (2, 'cherry')]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qK5gTj-tDBHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "# transform word count data into a sparse matrix representation suitable for use in machine learning models\n",
        "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vocabulary_size=1000):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "    def fit(self, X, y=None):\n",
        "        # calculates the most common words from the input word count data X and constructs a vocabulary based on the most common words\n",
        "        total_count = Counter()\n",
        "        for word_count in X:\n",
        "            for word, count in word_count.items():\n",
        "                #  increments the count of the word in the total_count Counter object\n",
        "                #  min(count, 10) ensures that the count of each word does not exceed 10\n",
        "                total_count[word] += min(count, 10)\n",
        "                #  to limit the impact of very high frequency words in the vocabulary\n",
        "\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        #  most_common method returns a list of tuples containing the most common words and their counts, ordered by frequency\n",
        "        # here, sliced to include only the most common words up to the specified vocabulary_size\n",
        "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
        "        # Using a dictionary comprehension, self.vocabulary_ is constructed.\n",
        "        # For each word and its count in the most common words list, a key-value pair is created, where the word is the key and the index (offset by 1) in the list is the value.\n",
        "        # The index offset by 1 is used to reserve index 0 for out-of-vocabulary or unknown words.\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        # converts the input word count data X into a sparse matrix representation\n",
        "        # Each row of the sparse matrix corresponds to the word counts for a specific input, and each column represents a word in the vocabulary\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))\n",
        "                # self.vocabulary_.get(word, 0) is used to obtain the index of a word in the vocabulary. If the word is present in the vocabulary, its index is retrieved.\n",
        "                # If the word is not found, the default index of 0 is returned, representing an out-of-vocabulary or unknown word.\n",
        "                data.append(count)\n",
        "        return csr_matrix((data, (rows, cols)),\n",
        "                          shape=(len(X), self.vocabulary_size + 1))\n",
        "        # returns a CSR matrix, where each row represents the word counts for a specific input, and each column represents a word in the vocabulary\n"
      ],
      "metadata": {
        "id": "oFKIUsN0llX8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the transformers:\n",
        "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
        "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
        "X_few_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqcDe1r8lw08",
        "outputId": "4f8bb029-4869-4cd4-c95f-0fa31a360ab5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 20 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_few_vectors.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6ew0YyRlx1s",
        "outputId": "228cdeab-71f0-4f1c-815b-10144ec665b5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [99, 11,  9,  8,  3,  1,  3,  1,  3,  2,  3],\n",
              "       [67,  0,  1,  2,  3,  4,  1,  2,  0,  1,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does this matrix mean? Well, the 99 in the second row, first column, means that the second email contains 99 words that are not part of the vocabulary. The 11 next to it means that the first word in the vocabulary is present 11 times in this email. The 9 next to it means that the second word is present 9 times, and so on. You can look at the vocabulary to know which words we are talking about. The first word is \"the\", the second word is \"of\", etc."
      ],
      "metadata": {
        "id": "dszEHFoAD9xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_transformer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNbNo5sQl1LE",
        "outputId": "84470760-8414-4e1e-f240-d2b8a6b9e4f8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'of': 2,\n",
              " 'and': 3,\n",
              " 'to': 4,\n",
              " 'url': 5,\n",
              " 'all': 6,\n",
              " 'in': 7,\n",
              " 'christian': 8,\n",
              " 'on': 9,\n",
              " 'by': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the spam classifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "preprocess_pipeline = Pipeline([\n",
        "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
        "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
        "])\n",
        "\n",
        "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "f7Leks9Dl3Ls"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3)\n",
        "score.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XXNYmHJl687",
        "outputId": "e4c6ed33-c534-4d19-f057-57b679e843b4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9858333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# precision/recall we get on the test set:\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
        "\n",
        "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_clf.fit(X_train_transformed, y_train)\n",
        "\n",
        "y_pred = log_clf.predict(X_test_transformed)\n",
        "\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.2%}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsVBgRAFmLRV",
        "outputId": "a76afb1a-b0ab-4346-d349-5ad6eeeb06c2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 96.88%\n",
            "Recall: 97.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g9V6hduMmOOk"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}